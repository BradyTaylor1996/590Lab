{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import pytorch dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# You cannot change this line.\n",
    "from tools.dataloader import CIFAR10,CIFAR10_test\n",
    "#from tools.dataloader import CIFAR10_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network module:\n",
    "class MyDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.conv7 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.conv8 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.conv9 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.conv10 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        self.conv11 = nn.Conv2d(16, 32, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.9)\n",
    "        self.bn2 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.9)\n",
    "        self.bn3 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.9)\n",
    "        self.bn4 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.9)\n",
    "        self.bn5 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9)\n",
    "        self.bn6 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9)\n",
    "        self.bn7 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9)\n",
    "        self.bn8 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9)\n",
    "        self.bn9 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9)\n",
    "        self.bn10 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.9)\n",
    "        self.fc1 = nn.Linear(32*16*16, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output1 = F.relu(self.bn1(self.conv1(x)))\n",
    "        output1 = F.relu(self.bn2(self.conv2(output1)))\n",
    "        \n",
    "        output2 = F.relu(self.bn3(self.conv3(output1)))\n",
    "        output2 = F.relu(self.bn4(output1 + self.conv4(output2)))\n",
    "        \n",
    "        output3 = F.relu(self.bn5(self.conv5(output2)))\n",
    "        output3 = F.relu(self.bn6(self.conv11(output2) + self.conv6(output3)))\n",
    "        \n",
    "        output4 = F.relu(self.bn7(self.conv7(output3)))\n",
    "        output4 = F.relu(self.bn8(output3 + self.conv8(output4)))\n",
    "        \n",
    "        output5 = F.relu(self.bn9(self.conv9(output4)))\n",
    "        output5 = F.relu(self.bn10(output4 + self.conv10(output5)))\n",
    "        \n",
    "        output = F.avg_pool2d(output5,2)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = F.relu(self.fc1(output))\n",
    "        output = self.fc2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 100\n",
    "INITIAL_LR = 0.01\n",
    "MOMENTUM = 0.9\n",
    "REG = 1e-5\n",
    "EPOCHS = 100\n",
    "DATAROOT = \"./data\"\n",
    "CHECKPOINT_PATH = \"./saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify preprocessing function.\n",
    "# Reference mean/std value for \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, 4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Using downloaded and verified file: ./data/cifar10_trainval.tar.gz\n",
      "Extracting ./data/cifar10_trainval.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Call the dataset Loader\n",
    "trainset = CIFAR10(root=DATAROOT, train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=1)\n",
    "valset = CIFAR10(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)\n",
    "testset = CIFAR10_test(root=DATAROOT, train=False, download=True, transform=transform_val)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=VAL_BATCH_SIZE, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU...\n"
     ]
    }
   ],
   "source": [
    "# Specify the device for computation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MyDNN()\n",
    "net = net.to(device)\n",
    "if device =='cuda':\n",
    "    print(\"Train on GPU...\")\n",
    "else:\n",
    "    print(\"Train on CPU...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './saved_model/model.h5'\n",
      "Checkpoint not found.\n",
      "Training from scratch ...\n",
      "Starting from learning rate 0.010000:\n"
     ]
    }
   ],
   "source": [
    "# FLAG for loading the pretrained model\n",
    "TRAIN_FROM_SCRATCH = False\n",
    "# Code for loading checkpoint and recover epoch id.\n",
    "CKPT_PATH = \"./saved_model/model.h5\"\n",
    "def get_checkpoint(ckpt_path):\n",
    "    try:\n",
    "        ckpt = torch.load(ckpt_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    return ckpt\n",
    "\n",
    "ckpt = get_checkpoint(CKPT_PATH)\n",
    "if ckpt is None or TRAIN_FROM_SCRATCH:\n",
    "    if not TRAIN_FROM_SCRATCH:\n",
    "        print(\"Checkpoint not found.\")\n",
    "    print(\"Training from scratch ...\")\n",
    "    start_epoch = 0\n",
    "    current_learning_rate = INITIAL_LR\n",
    "else:\n",
    "    print(\"Successfully loaded checkpoint: %s\" %CKPT_PATH)\n",
    "    net.load_state_dict(ckpt['net'])\n",
    "    start_epoch = ckpt['epoch'] + 1\n",
    "    current_learning_rate = ckpt['lr']\n",
    "    print(\"Starting from epoch %d \" %start_epoch)\n",
    "\n",
    "print(\"Starting from learning rate %f:\" %current_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss function and specify regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Add optimizer\n",
    "optimizer = optim.SGD(net.parameters(), INITIAL_LR, MOMENTUM, REG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-02 00:47:40.040962\n",
      "Epoch 0:\n",
      "176\n",
      "Initial loss: 2.3203\n",
      "Training loss: 1.7327, Training accuracy: 0.4167\n",
      "2019-10-02 00:47:52.035882\n",
      "Validation...\n",
      "Validation loss: 1.2706, Validation accuracy: 0.5418\n",
      "Saving ...\n",
      "2019-10-02 00:47:53.346318\n",
      "Epoch 1:\n",
      "176\n",
      "Training loss: 1.2502, Training accuracy: 0.5602\n",
      "2019-10-02 00:48:05.754204\n",
      "Validation...\n",
      "Validation loss: 1.0536, Validation accuracy: 0.6212\n",
      "Saving ...\n",
      "2019-10-02 00:48:07.062544\n",
      "Epoch 2:\n",
      "176\n",
      "Training loss: 1.0337, Training accuracy: 0.6357\n",
      "2019-10-02 00:48:19.206401\n",
      "Validation...\n",
      "Validation loss: 0.9076, Validation accuracy: 0.6756\n",
      "Saving ...\n",
      "2019-10-02 00:48:20.535266\n",
      "Epoch 3:\n",
      "176\n",
      "Training loss: 0.9005, Training accuracy: 0.6784\n",
      "2019-10-02 00:48:32.637473\n",
      "Validation...\n",
      "Validation loss: 0.8557, Validation accuracy: 0.6990\n",
      "Current learning rate has decayed to 0.100000\n",
      "Saving ...\n",
      "2019-10-02 00:48:33.933923\n",
      "Epoch 4:\n",
      "176\n",
      "Training loss: 1.3401, Training accuracy: 0.5676\n",
      "2019-10-02 00:48:45.998281\n",
      "Validation...\n",
      "Validation loss: 0.9602, Validation accuracy: 0.6564\n",
      "2019-10-02 00:48:47.117589\n",
      "Epoch 5:\n",
      "176\n",
      "Training loss: 0.9147, Training accuracy: 0.6820\n",
      "2019-10-02 00:48:59.505417\n",
      "Validation...\n",
      "Validation loss: 0.8184, Validation accuracy: 0.7056\n",
      "Saving ...\n",
      "2019-10-02 00:49:00.836292\n",
      "Epoch 6:\n",
      "176\n",
      "Training loss: 0.7914, Training accuracy: 0.7206\n",
      "2019-10-02 00:49:13.147982\n",
      "Validation...\n",
      "Validation loss: 0.7393, Validation accuracy: 0.7410\n",
      "Saving ...\n",
      "2019-10-02 00:49:14.466363\n",
      "Epoch 7:\n",
      "176\n",
      "Training loss: 0.7156, Training accuracy: 0.7470\n",
      "2019-10-02 00:49:27.596204\n",
      "Validation...\n",
      "Validation loss: 0.7054, Validation accuracy: 0.7596\n",
      "Saving ...\n",
      "2019-10-02 00:49:28.935526\n",
      "Epoch 8:\n",
      "176\n",
      "Training loss: 0.6819, Training accuracy: 0.7623\n",
      "2019-10-02 00:49:40.864999\n",
      "Validation...\n",
      "Validation loss: 0.6592, Validation accuracy: 0.7684\n",
      "Saving ...\n",
      "2019-10-02 00:49:42.171822\n",
      "Epoch 9:\n",
      "176\n",
      "Training loss: 0.6441, Training accuracy: 0.7756\n",
      "2019-10-02 00:49:54.187898\n",
      "Validation...\n",
      "Validation loss: 0.6619, Validation accuracy: 0.7654\n",
      "2019-10-02 00:49:55.314445\n",
      "Epoch 10:\n",
      "176\n",
      "Training loss: 0.6012, Training accuracy: 0.7912\n",
      "2019-10-02 00:50:07.570269\n",
      "Validation...\n",
      "Validation loss: 0.6317, Validation accuracy: 0.7832\n",
      "Current learning rate has decayed to 0.090000\n",
      "Saving ...\n",
      "2019-10-02 00:50:08.957234\n",
      "Epoch 11:\n",
      "176\n",
      "Training loss: 0.5615, Training accuracy: 0.8052\n",
      "2019-10-02 00:50:21.006084\n",
      "Validation...\n",
      "Validation loss: 0.5923, Validation accuracy: 0.7968\n",
      "Saving ...\n",
      "2019-10-02 00:50:22.391793\n",
      "Epoch 12:\n",
      "176\n",
      "Training loss: 0.5411, Training accuracy: 0.8107\n",
      "2019-10-02 00:50:35.033014\n",
      "Validation...\n",
      "Validation loss: 0.5699, Validation accuracy: 0.8050\n",
      "Saving ...\n",
      "2019-10-02 00:50:36.405026\n",
      "Epoch 13:\n",
      "176\n",
      "Training loss: 0.5050, Training accuracy: 0.8237\n",
      "2019-10-02 00:50:48.478277\n",
      "Validation...\n",
      "Validation loss: 0.5547, Validation accuracy: 0.8118\n",
      "Saving ...\n",
      "2019-10-02 00:50:49.754340\n",
      "Epoch 14:\n",
      "176\n",
      "Training loss: 0.4836, Training accuracy: 0.8314\n",
      "2019-10-02 00:51:01.602558\n",
      "Validation...\n",
      "Validation loss: 0.5522, Validation accuracy: 0.8128\n",
      "Saving ...\n",
      "2019-10-02 00:51:03.001395\n",
      "Epoch 15:\n",
      "176\n",
      "Training loss: 0.4698, Training accuracy: 0.8365\n",
      "2019-10-02 00:51:15.524981\n",
      "Validation...\n",
      "Validation loss: 0.5426, Validation accuracy: 0.8152\n",
      "Saving ...\n",
      "2019-10-02 00:51:16.822242\n",
      "Epoch 16:\n",
      "176\n",
      "Training loss: 0.4551, Training accuracy: 0.8403\n",
      "2019-10-02 00:51:28.820046\n",
      "Validation...\n",
      "Validation loss: 0.5699, Validation accuracy: 0.8120\n",
      "2019-10-02 00:51:30.032993\n",
      "Epoch 17:\n",
      "176\n",
      "Training loss: 0.4474, Training accuracy: 0.8447\n",
      "2019-10-02 00:51:42.081872\n",
      "Validation...\n",
      "Validation loss: 0.5166, Validation accuracy: 0.8300\n",
      "Saving ...\n",
      "2019-10-02 00:51:43.353785\n",
      "Epoch 18:\n",
      "176\n",
      "Training loss: 0.4245, Training accuracy: 0.8511\n",
      "2019-10-02 00:51:55.288995\n",
      "Validation...\n",
      "Validation loss: 0.5297, Validation accuracy: 0.8234\n",
      "2019-10-02 00:51:56.386456\n",
      "Epoch 19:\n",
      "176\n",
      "Training loss: 0.4162, Training accuracy: 0.8541\n",
      "2019-10-02 00:52:08.874632\n",
      "Validation...\n",
      "Validation loss: 0.4927, Validation accuracy: 0.8360\n",
      "Saving ...\n",
      "2019-10-02 00:52:10.370315\n",
      "Epoch 20:\n",
      "176\n",
      "Training loss: 0.3884, Training accuracy: 0.8612\n",
      "2019-10-02 00:52:22.430842\n",
      "Validation...\n",
      "Validation loss: 0.4903, Validation accuracy: 0.8416\n",
      "Current learning rate has decayed to 0.081000\n",
      "Saving ...\n",
      "2019-10-02 00:52:23.736211\n",
      "Epoch 21:\n",
      "176\n",
      "Training loss: 0.3699, Training accuracy: 0.8681\n",
      "2019-10-02 00:52:35.760476\n",
      "Validation...\n",
      "Validation loss: 0.4915, Validation accuracy: 0.8348\n",
      "2019-10-02 00:52:36.876266\n",
      "Epoch 22:\n",
      "176\n",
      "Training loss: 0.3687, Training accuracy: 0.8683\n",
      "2019-10-02 00:52:48.772015\n",
      "Validation...\n",
      "Validation loss: 0.5179, Validation accuracy: 0.8272\n",
      "2019-10-02 00:52:49.916181\n",
      "Epoch 23:\n",
      "176\n",
      "Training loss: 0.3535, Training accuracy: 0.8756\n",
      "2019-10-02 00:53:01.990266\n",
      "Validation...\n",
      "Validation loss: 0.5054, Validation accuracy: 0.8364\n",
      "2019-10-02 00:53:03.109199\n",
      "Epoch 24:\n",
      "176\n",
      "Training loss: 0.3454, Training accuracy: 0.8795\n",
      "2019-10-02 00:53:15.529759\n",
      "Validation...\n",
      "Validation loss: 0.4995, Validation accuracy: 0.8420\n",
      "Saving ...\n",
      "2019-10-02 00:53:17.035515\n",
      "Epoch 25:\n",
      "176\n",
      "Training loss: 0.3341, Training accuracy: 0.8825\n",
      "2019-10-02 00:53:29.031389\n",
      "Validation...\n",
      "Validation loss: 0.5274, Validation accuracy: 0.8388\n",
      "2019-10-02 00:53:30.150883\n",
      "Epoch 26:\n",
      "176\n",
      "Training loss: 0.3357, Training accuracy: 0.8824\n",
      "2019-10-02 00:53:42.573162\n",
      "Validation...\n",
      "Validation loss: 0.5067, Validation accuracy: 0.8380\n",
      "2019-10-02 00:53:43.880045\n",
      "Epoch 27:\n",
      "176\n",
      "Training loss: 0.3262, Training accuracy: 0.8868\n",
      "2019-10-02 00:53:56.308292\n",
      "Validation...\n",
      "Validation loss: 0.5045, Validation accuracy: 0.8362\n",
      "2019-10-02 00:53:57.604572\n",
      "Epoch 28:\n",
      "176\n",
      "Training loss: 0.3201, Training accuracy: 0.8893\n",
      "2019-10-02 00:54:09.727601\n",
      "Validation...\n",
      "Validation loss: 0.4914, Validation accuracy: 0.8400\n",
      "2019-10-02 00:54:10.848041\n",
      "Epoch 29:\n",
      "176\n",
      "Training loss: 0.3079, Training accuracy: 0.8912\n",
      "2019-10-02 00:54:23.158498\n",
      "Validation...\n",
      "Validation loss: 0.4966, Validation accuracy: 0.8466\n",
      "Saving ...\n",
      "2019-10-02 00:54:24.533427\n",
      "Epoch 30:\n",
      "176\n",
      "Training loss: 0.3012, Training accuracy: 0.8943\n",
      "2019-10-02 00:54:37.591648\n",
      "Validation...\n",
      "Validation loss: 0.4723, Validation accuracy: 0.8480\n",
      "Current learning rate has decayed to 0.072900\n",
      "Saving ...\n",
      "2019-10-02 00:54:38.999631\n",
      "Epoch 31:\n",
      "176\n",
      "Training loss: 0.2862, Training accuracy: 0.9016\n",
      "2019-10-02 00:54:51.199307\n",
      "Validation...\n",
      "Validation loss: 0.4945, Validation accuracy: 0.8524\n",
      "Saving ...\n",
      "2019-10-02 00:54:52.538861\n",
      "Epoch 32:\n",
      "176\n",
      "Training loss: 0.2790, Training accuracy: 0.9025\n",
      "2019-10-02 00:55:04.832986\n",
      "Validation...\n",
      "Validation loss: 0.4809, Validation accuracy: 0.8484\n",
      "2019-10-02 00:55:05.987412\n",
      "Epoch 33:\n",
      "176\n",
      "Training loss: 0.2714, Training accuracy: 0.9062\n",
      "2019-10-02 00:55:18.530021\n",
      "Validation...\n",
      "Validation loss: 0.4625, Validation accuracy: 0.8546\n",
      "Saving ...\n",
      "2019-10-02 00:55:20.007239\n",
      "Epoch 34:\n",
      "176\n",
      "Training loss: 0.2642, Training accuracy: 0.9084\n",
      "2019-10-02 00:55:33.088995\n",
      "Validation...\n",
      "Validation loss: 0.4927, Validation accuracy: 0.8494\n",
      "2019-10-02 00:55:34.212677\n",
      "Epoch 35:\n",
      "176\n",
      "Training loss: 0.2608, Training accuracy: 0.9094\n",
      "2019-10-02 00:55:46.073434\n",
      "Validation...\n",
      "Validation loss: 0.5054, Validation accuracy: 0.8430\n",
      "2019-10-02 00:55:47.493276\n",
      "Epoch 36:\n",
      "176\n",
      "Training loss: 0.2567, Training accuracy: 0.9108\n",
      "2019-10-02 00:56:01.000989\n",
      "Validation...\n",
      "Validation loss: 0.4873, Validation accuracy: 0.8480\n",
      "2019-10-02 00:56:02.130694\n",
      "Epoch 37:\n",
      "176\n",
      "Training loss: 0.2473, Training accuracy: 0.9113\n",
      "2019-10-02 00:56:14.164987\n",
      "Validation...\n",
      "Validation loss: 0.5191, Validation accuracy: 0.8488\n",
      "2019-10-02 00:56:15.352998\n",
      "Epoch 38:\n",
      "176\n",
      "Training loss: 0.2490, Training accuracy: 0.9135\n",
      "2019-10-02 00:56:27.536986\n",
      "Validation...\n",
      "Validation loss: 0.5215, Validation accuracy: 0.8498\n",
      "2019-10-02 00:56:28.630120\n",
      "Epoch 39:\n",
      "176\n",
      "Training loss: 0.2382, Training accuracy: 0.9157\n",
      "2019-10-02 00:56:41.022504\n",
      "Validation...\n",
      "Validation loss: 0.5077, Validation accuracy: 0.8520\n",
      "2019-10-02 00:56:42.188981\n",
      "Epoch 40:\n",
      "176\n",
      "Training loss: 0.2386, Training accuracy: 0.9155\n",
      "2019-10-02 00:56:54.325009\n",
      "Validation...\n",
      "Validation loss: 0.5213, Validation accuracy: 0.8472\n",
      "Current learning rate has decayed to 0.065610\n",
      "2019-10-02 00:56:55.432365\n",
      "Epoch 41:\n",
      "176\n",
      "Training loss: 0.2155, Training accuracy: 0.9222\n",
      "2019-10-02 00:57:07.807597\n",
      "Validation...\n",
      "Validation loss: 0.4747, Validation accuracy: 0.8566\n",
      "Saving ...\n",
      "2019-10-02 00:57:09.178468\n",
      "Epoch 42:\n",
      "176\n",
      "Training loss: 0.2054, Training accuracy: 0.9256\n",
      "2019-10-02 00:57:21.407725\n",
      "Validation...\n",
      "Validation loss: 0.5102, Validation accuracy: 0.8566\n",
      "2019-10-02 00:57:22.565244\n",
      "Epoch 43:\n",
      "176\n",
      "Training loss: 0.2067, Training accuracy: 0.9275\n",
      "2019-10-02 00:57:34.629717\n",
      "Validation...\n",
      "Validation loss: 0.5044, Validation accuracy: 0.8506\n",
      "2019-10-02 00:57:35.847355\n",
      "Epoch 44:\n",
      "176\n",
      "Training loss: 0.2058, Training accuracy: 0.9282\n",
      "2019-10-02 00:57:47.770946\n",
      "Validation...\n",
      "Validation loss: 0.5047, Validation accuracy: 0.8538\n",
      "2019-10-02 00:57:48.932584\n",
      "Epoch 45:\n",
      "176\n",
      "Training loss: 0.1973, Training accuracy: 0.9280\n",
      "2019-10-02 00:58:01.509315\n",
      "Validation...\n",
      "Validation loss: 0.5101, Validation accuracy: 0.8596\n",
      "Saving ...\n",
      "2019-10-02 00:58:02.801633\n",
      "Epoch 46:\n",
      "176\n",
      "Training loss: 0.1949, Training accuracy: 0.9314\n",
      "2019-10-02 00:58:15.089185\n",
      "Validation...\n",
      "Validation loss: 0.5086, Validation accuracy: 0.8538\n",
      "2019-10-02 00:58:16.225595\n",
      "Epoch 47:\n",
      "176\n",
      "Training loss: 0.2010, Training accuracy: 0.9304\n",
      "2019-10-02 00:58:28.322480\n",
      "Validation...\n",
      "Validation loss: 0.5090, Validation accuracy: 0.8562\n",
      "2019-10-02 00:58:29.582606\n",
      "Epoch 48:\n",
      "176\n",
      "Training loss: 0.1918, Training accuracy: 0.9313\n",
      "2019-10-02 00:58:41.885167\n",
      "Validation...\n",
      "Validation loss: 0.5214, Validation accuracy: 0.8558\n",
      "2019-10-02 00:58:43.041739\n",
      "Epoch 49:\n",
      "176\n",
      "Training loss: 0.1890, Training accuracy: 0.9345\n",
      "2019-10-02 00:58:54.954355\n",
      "Validation...\n",
      "Validation loss: 0.5288, Validation accuracy: 0.8504\n",
      "2019-10-02 00:58:56.192751\n",
      "Epoch 50:\n",
      "176\n",
      "Training loss: 0.1886, Training accuracy: 0.9344\n",
      "2019-10-02 00:59:08.486818\n",
      "Validation...\n",
      "Validation loss: 0.5280, Validation accuracy: 0.8550\n",
      "Current learning rate has decayed to 0.059049\n",
      "2019-10-02 00:59:09.656933\n",
      "Epoch 51:\n",
      "176\n",
      "Training loss: 0.1831, Training accuracy: 0.9363\n",
      "2019-10-02 00:59:22.585002\n",
      "Validation...\n",
      "Validation loss: 0.5287, Validation accuracy: 0.8562\n",
      "2019-10-02 00:59:23.753148\n",
      "Epoch 52:\n",
      "176\n",
      "Training loss: 0.1727, Training accuracy: 0.9392\n",
      "2019-10-02 00:59:36.886366\n",
      "Validation...\n",
      "Validation loss: 0.5250, Validation accuracy: 0.8562\n",
      "2019-10-02 00:59:37.985366\n",
      "Epoch 53:\n",
      "176\n",
      "Training loss: 0.1672, Training accuracy: 0.9418\n",
      "2019-10-02 00:59:52.052286\n",
      "Validation...\n",
      "Validation loss: 0.5217, Validation accuracy: 0.8592\n",
      "2019-10-02 00:59:53.420499\n",
      "Epoch 54:\n",
      "176\n",
      "Training loss: 0.1601, Training accuracy: 0.9422\n",
      "2019-10-02 01:00:05.683423\n",
      "Validation...\n",
      "Validation loss: 0.5241, Validation accuracy: 0.8604\n",
      "Saving ...\n",
      "2019-10-02 01:00:07.077170\n",
      "Epoch 55:\n",
      "176\n",
      "Training loss: 0.1569, Training accuracy: 0.9454\n",
      "2019-10-02 01:00:21.196093\n",
      "Validation...\n",
      "Validation loss: 0.5245, Validation accuracy: 0.8580\n",
      "2019-10-02 01:00:22.341718\n",
      "Epoch 56:\n",
      "176\n",
      "Training loss: 0.1600, Training accuracy: 0.9442\n",
      "2019-10-02 01:00:34.017907\n",
      "Validation...\n",
      "Validation loss: 0.5743, Validation accuracy: 0.8590\n",
      "2019-10-02 01:00:35.194685\n",
      "Epoch 57:\n",
      "176\n",
      "Training loss: 0.1541, Training accuracy: 0.9468\n",
      "2019-10-02 01:00:47.038786\n",
      "Validation...\n",
      "Validation loss: 0.5546, Validation accuracy: 0.8588\n",
      "2019-10-02 01:00:48.118461\n",
      "Epoch 58:\n",
      "176\n",
      "Training loss: 0.1557, Training accuracy: 0.9455\n",
      "2019-10-02 01:01:00.697492\n",
      "Validation...\n",
      "Validation loss: 0.5345, Validation accuracy: 0.8614\n",
      "Saving ...\n",
      "2019-10-02 01:01:02.022931\n",
      "Epoch 59:\n",
      "176\n",
      "Training loss: 0.1452, Training accuracy: 0.9467\n",
      "2019-10-02 01:01:13.915550\n",
      "Validation...\n",
      "Validation loss: 0.5553, Validation accuracy: 0.8548\n",
      "2019-10-02 01:01:15.021020\n",
      "Epoch 60:\n",
      "176\n",
      "Training loss: 0.1509, Training accuracy: 0.9471\n",
      "2019-10-02 01:01:27.228169\n",
      "Validation...\n",
      "Validation loss: 0.5720, Validation accuracy: 0.8540\n",
      "Current learning rate has decayed to 0.053144\n",
      "2019-10-02 01:01:28.341568\n",
      "Epoch 61:\n",
      "176\n",
      "Training loss: 0.1444, Training accuracy: 0.9509\n",
      "2019-10-02 01:01:40.999555\n",
      "Validation...\n",
      "Validation loss: 0.5704, Validation accuracy: 0.8554\n",
      "2019-10-02 01:01:42.286482\n",
      "Epoch 62:\n",
      "176\n",
      "Training loss: 0.1323, Training accuracy: 0.9535\n",
      "2019-10-02 01:01:54.836473\n",
      "Validation...\n",
      "Validation loss: 0.5718, Validation accuracy: 0.8608\n",
      "2019-10-02 01:01:55.967465\n",
      "Epoch 63:\n",
      "176\n",
      "Training loss: 0.1390, Training accuracy: 0.9514\n",
      "2019-10-02 01:02:08.378190\n",
      "Validation...\n",
      "Validation loss: 0.5635, Validation accuracy: 0.8620\n",
      "Saving ...\n",
      "2019-10-02 01:02:09.688361\n",
      "Epoch 64:\n",
      "176\n",
      "Training loss: 0.1274, Training accuracy: 0.9557\n",
      "2019-10-02 01:02:22.346947\n",
      "Validation...\n",
      "Validation loss: 0.5414, Validation accuracy: 0.8586\n",
      "2019-10-02 01:02:23.496980\n",
      "Epoch 65:\n",
      "176\n",
      "Training loss: 0.1287, Training accuracy: 0.9551\n",
      "2019-10-02 01:02:36.004087\n",
      "Validation...\n",
      "Validation loss: 0.5448, Validation accuracy: 0.8610\n",
      "2019-10-02 01:02:37.088340\n",
      "Epoch 66:\n",
      "176\n",
      "Training loss: 0.1370, Training accuracy: 0.9538\n",
      "2019-10-02 01:02:49.011951\n",
      "Validation...\n",
      "Validation loss: 0.5537, Validation accuracy: 0.8590\n",
      "2019-10-02 01:02:50.117369\n",
      "Epoch 67:\n",
      "176\n",
      "Training loss: 0.1263, Training accuracy: 0.9550\n",
      "2019-10-02 01:03:02.090466\n",
      "Validation...\n",
      "Validation loss: 0.5570, Validation accuracy: 0.8630\n",
      "Saving ...\n",
      "2019-10-02 01:03:03.375703\n",
      "Epoch 68:\n",
      "176\n",
      "Training loss: 0.1230, Training accuracy: 0.9555\n",
      "2019-10-02 01:03:15.747570\n",
      "Validation...\n",
      "Validation loss: 0.5814, Validation accuracy: 0.8610\n",
      "2019-10-02 01:03:16.868672\n",
      "Epoch 69:\n",
      "176\n",
      "Training loss: 0.1175, Training accuracy: 0.9589\n",
      "2019-10-02 01:03:28.792819\n",
      "Validation...\n",
      "Validation loss: 0.5601, Validation accuracy: 0.8638\n",
      "Saving ...\n",
      "2019-10-02 01:03:30.133247\n",
      "Epoch 70:\n",
      "176\n",
      "Training loss: 0.1144, Training accuracy: 0.9583\n",
      "2019-10-02 01:03:42.408985\n",
      "Validation...\n",
      "Validation loss: 0.5882, Validation accuracy: 0.8602\n",
      "Current learning rate has decayed to 0.047830\n",
      "2019-10-02 01:03:43.577045\n",
      "Epoch 71:\n",
      "176\n",
      "Training loss: 0.1109, Training accuracy: 0.9614\n",
      "2019-10-02 01:03:55.882327\n",
      "Validation...\n",
      "Validation loss: 0.5863, Validation accuracy: 0.8672\n",
      "Saving ...\n",
      "2019-10-02 01:03:57.357460\n",
      "Epoch 72:\n",
      "176\n",
      "Training loss: 0.1131, Training accuracy: 0.9613\n",
      "2019-10-02 01:04:09.686452\n",
      "Validation...\n",
      "Validation loss: 0.5706, Validation accuracy: 0.8642\n",
      "2019-10-02 01:04:10.826995\n",
      "Epoch 73:\n",
      "176\n",
      "Training loss: 0.1017, Training accuracy: 0.9646\n",
      "2019-10-02 01:04:24.624871\n",
      "Validation...\n",
      "Validation loss: 0.5626, Validation accuracy: 0.8622\n",
      "2019-10-02 01:04:25.755307\n",
      "Epoch 74:\n",
      "176\n",
      "Training loss: 0.1009, Training accuracy: 0.9651\n",
      "2019-10-02 01:04:37.857234\n",
      "Validation...\n",
      "Validation loss: 0.6011, Validation accuracy: 0.8648\n",
      "2019-10-02 01:04:39.097860\n",
      "Epoch 75:\n",
      "176\n",
      "Training loss: 0.0949, Training accuracy: 0.9656\n",
      "2019-10-02 01:04:51.291357\n",
      "Validation...\n",
      "Validation loss: 0.6330, Validation accuracy: 0.8604\n",
      "2019-10-02 01:04:52.434320\n",
      "Epoch 76:\n",
      "176\n",
      "Training loss: 0.1013, Training accuracy: 0.9637\n",
      "2019-10-02 01:05:04.647973\n",
      "Validation...\n",
      "Validation loss: 0.5953, Validation accuracy: 0.8688\n",
      "Saving ...\n",
      "2019-10-02 01:05:05.983360\n",
      "Epoch 77:\n",
      "176\n",
      "Training loss: 0.1030, Training accuracy: 0.9648\n",
      "2019-10-02 01:05:18.507551\n",
      "Validation...\n",
      "Validation loss: 0.6172, Validation accuracy: 0.8602\n",
      "2019-10-02 01:05:19.660338\n",
      "Epoch 78:\n",
      "176\n",
      "Training loss: 0.0989, Training accuracy: 0.9664\n",
      "2019-10-02 01:05:31.796455\n",
      "Validation...\n",
      "Validation loss: 0.5971, Validation accuracy: 0.8664\n",
      "2019-10-02 01:05:32.955536\n",
      "Epoch 79:\n",
      "176\n",
      "Training loss: 0.0929, Training accuracy: 0.9674\n",
      "2019-10-02 01:05:45.055992\n",
      "Validation...\n",
      "Validation loss: 0.5940, Validation accuracy: 0.8632\n",
      "2019-10-02 01:05:46.174476\n",
      "Epoch 80:\n",
      "176\n",
      "Training loss: 0.0909, Training accuracy: 0.9690\n",
      "2019-10-02 01:05:58.555654\n",
      "Validation...\n",
      "Validation loss: 0.6254, Validation accuracy: 0.8658\n",
      "Current learning rate has decayed to 0.043047\n",
      "2019-10-02 01:05:59.723653\n",
      "Epoch 81:\n",
      "176\n",
      "Training loss: 0.0886, Training accuracy: 0.9697\n",
      "2019-10-02 01:06:11.736683\n",
      "Validation...\n",
      "Validation loss: 0.5850, Validation accuracy: 0.8696\n",
      "Saving ...\n",
      "2019-10-02 01:06:13.021072\n",
      "Epoch 82:\n",
      "176\n",
      "Training loss: 0.0838, Training accuracy: 0.9714\n",
      "2019-10-02 01:06:25.333349\n",
      "Validation...\n",
      "Validation loss: 0.6204, Validation accuracy: 0.8686\n",
      "2019-10-02 01:06:26.468131\n",
      "Epoch 83:\n",
      "176\n",
      "Training loss: 0.0889, Training accuracy: 0.9701\n",
      "2019-10-02 01:06:38.528964\n",
      "Validation...\n",
      "Validation loss: 0.6116, Validation accuracy: 0.8624\n",
      "2019-10-02 01:06:39.664031\n",
      "Epoch 84:\n",
      "176\n",
      "Training loss: 0.0874, Training accuracy: 0.9702\n",
      "2019-10-02 01:06:51.712896\n",
      "Validation...\n",
      "Validation loss: 0.6188, Validation accuracy: 0.8664\n",
      "2019-10-02 01:06:52.822515\n",
      "Epoch 85:\n",
      "176\n",
      "Training loss: 0.0809, Training accuracy: 0.9721\n",
      "2019-10-02 01:07:04.698938\n",
      "Validation...\n",
      "Validation loss: 0.6119, Validation accuracy: 0.8710\n",
      "Saving ...\n",
      "2019-10-02 01:07:06.202699\n",
      "Epoch 86:\n",
      "176\n",
      "Training loss: 0.0819, Training accuracy: 0.9723\n",
      "2019-10-02 01:07:18.280290\n",
      "Validation...\n",
      "Validation loss: 0.6301, Validation accuracy: 0.8688\n",
      "2019-10-02 01:07:19.404992\n",
      "Epoch 87:\n",
      "176\n",
      "Training loss: 0.0847, Training accuracy: 0.9718\n",
      "2019-10-02 01:07:31.640486\n",
      "Validation...\n",
      "Validation loss: 0.6448, Validation accuracy: 0.8648\n",
      "2019-10-02 01:07:32.804993\n",
      "Epoch 88:\n",
      "176\n",
      "Training loss: 0.0722, Training accuracy: 0.9737\n",
      "2019-10-02 01:07:45.424720\n",
      "Validation...\n",
      "Validation loss: 0.6450, Validation accuracy: 0.8626\n",
      "2019-10-02 01:07:46.580736\n",
      "Epoch 89:\n",
      "176\n",
      "Training loss: 0.0828, Training accuracy: 0.9710\n",
      "2019-10-02 01:07:58.894893\n",
      "Validation...\n",
      "Validation loss: 0.6240, Validation accuracy: 0.8712\n",
      "Saving ...\n",
      "2019-10-02 01:08:00.316557\n",
      "Epoch 90:\n",
      "176\n",
      "Training loss: 0.0745, Training accuracy: 0.9737\n",
      "2019-10-02 01:08:12.805104\n",
      "Validation...\n",
      "Validation loss: 0.6453, Validation accuracy: 0.8688\n",
      "Current learning rate has decayed to 0.038742\n",
      "2019-10-02 01:08:14.255182\n",
      "Epoch 91:\n",
      "176\n",
      "Training loss: 0.0750, Training accuracy: 0.9741\n",
      "2019-10-02 01:08:26.612986\n",
      "Validation...\n",
      "Validation loss: 0.6340, Validation accuracy: 0.8718\n",
      "Saving ...\n",
      "2019-10-02 01:08:28.035252\n",
      "Epoch 92:\n",
      "176\n",
      "Training loss: 0.0651, Training accuracy: 0.9776\n",
      "2019-10-02 01:08:40.336979\n",
      "Validation...\n",
      "Validation loss: 0.6510, Validation accuracy: 0.8722\n",
      "Saving ...\n",
      "2019-10-02 01:08:41.897324\n",
      "Epoch 93:\n",
      "176\n",
      "Training loss: 0.0728, Training accuracy: 0.9750\n",
      "2019-10-02 01:08:54.289719\n",
      "Validation...\n",
      "Validation loss: 0.6266, Validation accuracy: 0.8720\n",
      "2019-10-02 01:08:55.408963\n",
      "Epoch 94:\n",
      "176\n",
      "Training loss: 0.0695, Training accuracy: 0.9759\n",
      "2019-10-02 01:09:07.595942\n",
      "Validation...\n",
      "Validation loss: 0.6616, Validation accuracy: 0.8678\n",
      "2019-10-02 01:09:08.757227\n",
      "Epoch 95:\n",
      "176\n",
      "Training loss: 0.0639, Training accuracy: 0.9778\n",
      "2019-10-02 01:09:21.196145\n",
      "Validation...\n",
      "Validation loss: 0.6478, Validation accuracy: 0.8698\n",
      "2019-10-02 01:09:22.296977\n",
      "Epoch 96:\n",
      "176\n",
      "Training loss: 0.0619, Training accuracy: 0.9781\n",
      "2019-10-02 01:09:34.528989\n",
      "Validation...\n",
      "Validation loss: 0.6374, Validation accuracy: 0.8724\n",
      "Saving ...\n",
      "2019-10-02 01:09:35.951816\n",
      "Epoch 97:\n",
      "176\n",
      "Training loss: 0.0638, Training accuracy: 0.9777\n",
      "2019-10-02 01:09:48.388849\n",
      "Validation...\n",
      "Validation loss: 0.6695, Validation accuracy: 0.8720\n",
      "2019-10-02 01:09:49.520983\n",
      "Epoch 98:\n",
      "176\n",
      "Training loss: 0.0669, Training accuracy: 0.9774\n",
      "2019-10-02 01:10:01.527030\n",
      "Validation...\n",
      "Validation loss: 0.6319, Validation accuracy: 0.8734\n",
      "Saving ...\n",
      "2019-10-02 01:10:02.846656\n",
      "Epoch 99:\n",
      "176\n",
      "Training loss: 0.0597, Training accuracy: 0.9794\n",
      "2019-10-02 01:10:16.001003\n",
      "Validation...\n",
      "Validation loss: 0.6664, Validation accuracy: 0.8688\n",
      "Optimization finished.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start the training/validation process\n",
    "# The process should take about 5 minutes on a GTX 1070-Ti\n",
    "# if the code is written efficiently.\n",
    "global_step = 0\n",
    "best_val_acc = 0\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "my_flag = 0\n",
    "\n",
    "for i in range(start_epoch, EPOCHS):\n",
    "    print(datetime.datetime.now())\n",
    "    # Switch to train mode\n",
    "    net.train()\n",
    "    print(\"Epoch %d:\" %i)\n",
    "\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    # Train the training dataset for 1 epoch.\n",
    "    print(len(trainloader))\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        if my_flag == 0:\n",
    "            print(\"Initial loss: %.4f\" %(loss))\n",
    "            my_flag = 1\n",
    "        # Now backward loss\n",
    "        loss.backward()\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        # Calculate accuracy\n",
    "        total_examples += len(outputs)\n",
    "        correct_examples += predicted.eq(targets).sum().item()\n",
    "\n",
    "        train_loss += loss\n",
    "\n",
    "        global_step += 1\n",
    "        if global_step % 100 == 0:\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "        pass\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Training loss: %.4f, Training accuracy: %.4f\" %(avg_loss, avg_acc))\n",
    "    training_accuracies.append(avg_acc)\n",
    "    print(datetime.datetime.now())\n",
    "    # Validate on the validation dataset\n",
    "    print(\"Validation...\")\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    # Disable gradient during validation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # Copy inputs to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Zero the gradient\n",
    "            optimizer.zero_grad()\n",
    "            # Generate output from the DNN.\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            # Calculate predicted labels\n",
    "            _, predicted = outputs.max(1)\n",
    "            # Calculate accuracy\n",
    "            total_examples += len(outputs)\n",
    "            correct_examples += predicted.eq(targets).sum().item()\n",
    "            val_loss += loss\n",
    "\n",
    "    avg_loss = val_loss / len(valloader)\n",
    "    avg_acc = correct_examples / total_examples\n",
    "    print(\"Validation loss: %.4f, Validation accuracy: %.4f\" % (avg_loss, avg_acc))\n",
    "    validation_accuracies.append(avg_acc)\n",
    "    \n",
    "    DECAY_EPOCHS = 10\n",
    "    DECAY = 0.9\n",
    "    \n",
    "    if i == 3:\n",
    "        current_learning_rate = 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "        \n",
    "    if i % DECAY_EPOCHS == 0 and i != 0:\n",
    "        current_learning_rate = current_learning_rate*DECAY\n",
    "        for param_group in optimizer.param_groups:\n",
    "            # Assign the learning rate parameter\n",
    "            param_group['lr'] = current_learning_rate\n",
    "        print(\"Current learning rate has decayed to %f\" %current_learning_rate)\n",
    "    \n",
    "    # Save for checkpoint\n",
    "    if avg_acc > best_val_acc:\n",
    "        best_val_acc = avg_acc\n",
    "        if not os.path.exists(CHECKPOINT_PATH):\n",
    "            os.makedirs(CHECKPOINT_PATH)\n",
    "        print(\"Saving ...\")\n",
    "        state = {'net': net.state_dict(),\n",
    "                 'epoch': i,\n",
    "                 'lr': current_learning_rate}\n",
    "        torch.save(state, os.path.join(CHECKPOINT_PATH, 'model.h5'))\n",
    "\n",
    "print(\"Optimization finished.\")\n",
    "all_epochs = list(range(EPOCHS))\n",
    "plt.plot(all_epochs, training_accuracies, label = 'Training Accuracy')\n",
    "plt.plot(all_epochs, validation_accuracies, label = 'Validation Accuracy')\n",
    "plt.ylim(0,1)\n",
    "plt.xlim(0,EPOCHS-1)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(len(testloader))\n",
    "predicted_labels = np.array([])\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        # Copy inputs to device\n",
    "        inputs = inputs.to(device)\n",
    "        # Zero the gradient\n",
    "        optimizer.zero_grad()\n",
    "        # Generate output from the DNN.\n",
    "        outputs = net(inputs)\n",
    "        # Calculate predicted labels\n",
    "        _, predicted = outputs.max(1)\n",
    "        predicted_labels = np.append(predicted_labels, predicted.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.000e+00 1.000e+00]\n",
      " [1.000e+00 0.000e+00]\n",
      " [2.000e+00 2.000e+00]\n",
      " ...\n",
      " [9.997e+03 4.000e+00]\n",
      " [9.998e+03 7.000e+00]\n",
      " [9.999e+03 3.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = np.append(range(10000), predicted_labels)\n",
    "predicted_labels = np.reshape(predicted_labels, (2,-1)).transpose()\n",
    "print(predicted_labels)\n",
    "\n",
    "np.savetxt('labels.csv', predicted_labels, delimiter=',', fmt=['%d', '%d'], header='id,Category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
